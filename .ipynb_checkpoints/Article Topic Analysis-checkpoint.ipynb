{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Article Topic Analysis\n",
    "## Overview\n",
    "The purpose of this repository is to analyse,visualize,and list the topics included in a set of articles.<br>\n",
    "These articles come from a dataset I have created over the last two years by collecting the URLs of articles of interest.\n",
    "## Process\n",
    "1. Data Extraction: Download text from each url (using BeautifuSoup)\n",
    "2. Data Cleansing: Clean dataset with common NLP techniques (stopword removal, lemmatization, etc.) (using NLTK)\n",
    "3. Saving Data: Saving the extracted dataset\n",
    "4. Data Visualization: Visualize with PCA or T-SNE (using Scikit-learn)\n",
    "5. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "import csv\n",
    "\n",
    "from nltk import sent_tokenize, word_tokenize, pos_tag\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Extraction and Cleansing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# text extraction\n",
    "def openUrlAndExtractText(url):\n",
    "    site  = urlopen(url)\n",
    "    soup = BeautifulSoup(site, \"html.parser\")\n",
    "    [s.extract() for s in soup(['style', 'script', '[document]', 'head', 'title'])]\n",
    "    text = soup.getText()\n",
    "    text = text.lower()\n",
    "    \n",
    "    return text\n",
    "    \n",
    "def extractSentencesFromText(text,minSizeOfSentence):\n",
    "    sentences = text.split(\"\\n\")\n",
    "    \n",
    "    #remove short sentences\n",
    "    sentences = [elem for elem in sentences if len(elem) >= minSizeOfSentence]\n",
    "    \n",
    "    extractedSentences = []\n",
    "    #checking for multiple sentences within each line with nltk\n",
    "    for sentence in sentences:\n",
    "        extractedSentences.extend(sent_tokenize(sentence))\n",
    "        \n",
    "    return extractedSentences\n",
    "\n",
    "def cleanSentences(sentences):\n",
    "    cleanSentences = []\n",
    "    for sentence in sentences:\n",
    "        if len(sentence) <= 1:\n",
    "            continue\n",
    "        cleanSentences.append(sentence.strip())\n",
    "        \n",
    "    return cleanSentences\n",
    "\n",
    "def turnSenteceListToWordCountDict(sentences):\n",
    "    wordCountDict = defaultdict(int)\n",
    "    for s in sentences:\n",
    "        words = word_tokenize(s)\n",
    "        for word in words:\n",
    "            wordCountDict[word] += 1\n",
    "    return wordCountDict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#loading dataset\n",
    "def getDatasetURLList(fileLoc):\n",
    "    dataUrls = []\n",
    "\n",
    "    with open(fileLoc) as csvfile:\n",
    "        readCSV = csv.reader(csvfile, delimiter=',')\n",
    "        for row in readCSV:\n",
    "            dataUrls.append(row[0])\n",
    "    return dataUrls\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLP Cleansing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#nlp cleaning methods\n",
    "def nlpCleaning(wordCountDict):\n",
    "    #remove stop words\n",
    "    wordCountDict = removeStopWords(wordCountDict)\n",
    "    \n",
    "    #lemmatize words\n",
    "    wordCountDict = lemmatizeWordCountDict(wordCountDict)\n",
    "    \n",
    "    return wordCountDict\n",
    "\n",
    "def removeStopWords(wordCountDict):\n",
    "    cleanedWordCountDict = defaultdict(int)\n",
    "    \n",
    "    for word,count in wordCountDict.items():\n",
    "        if word not in stopwords.words('english'):\n",
    "            cleanedWordCountDict[word] = count\n",
    "    \n",
    "    return cleanedWordCountDict\n",
    "        \n",
    "def lemmatizeWordCountDict(wordCountDict):\n",
    "    lemmatizedWordCountDict = defaultdict(int)\n",
    "    \n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    \n",
    "    for word,count in wordCountDict.items():\n",
    "        lemmatizedWordCountDict[lemmatizer.lemmatize(word, get_wordnet_pos(word))] += count\n",
    "    \n",
    "    return lemmatizedWordCountDict\n",
    "\n",
    "def get_wordnet_pos(word):\n",
    "    \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n",
    "    tag = pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "\n",
    "    return tag_dict.get(tag, wordnet.NOUN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running Data extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Input variables\n",
    "dataSetCSVLocation = 'Data/Article_urls_KeepTransfer0_5.csv'\n",
    "dataUrls = getDatasetURLList(dataSetCSVLocation)\n",
    "\n",
    "minSizeOfSentence = 2\n",
    "\n",
    "articlesPerSaveFile = 25\n",
    "\n",
    "#other variables\n",
    "wordCountDicts = []\n",
    "\n",
    "successes = 0\n",
    "errors = 0\n",
    "\n",
    "#extracting the data from each url\n",
    "for url in dataUrls:\n",
    "    try:\n",
    "        #extracting raw text from site\n",
    "        rawText =  openUrlAndExtractText(url)\n",
    "        #finding sentences in the text\n",
    "        rawSentences = extractSentencesFromText(rawText,minSizeOfSentence)\n",
    "        #cleaning sentences up\n",
    "        cleanedSentences = cleanSentences(rawSentences)\n",
    "\n",
    "        wordCountDict = turnSenteceListToWordCountDict(cleanedSentences)\n",
    "\n",
    "        cleanedWordCountDict = nlpCleaning(wordCountDict)\n",
    "\n",
    "        wordCountDicts.append(cleanedWordCountDict)\n",
    "        successes += 1\n",
    "        print(successes)\n",
    "        if successes % articlesPerSaveFile == 0:\n",
    "            with open('Data/Datafiles/'+successes+'.pickle', 'wb') as handle:\n",
    "                pickle.dump(wordCountDicts, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "                wordCountDicts = []\n",
    "            print(successes, \"saved\")\n",
    "    except:\n",
    "        errors += 1\n",
    "        print(url)\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
